import sys
import os
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from PyQt5.QtWidgets import (QApplication, QMainWindow, QVBoxLayout, QHBoxLayout,
                             QPushButton, QLabel, QFileDialog, QTextEdit, 
                             QWidget, QProgressBar, QTabWidget, QGroupBox)
from PyQt5.QtCore import QThread, pyqtSignal, Qt
import warnings
warnings.filterwarnings('ignore')
import sounddevice as sd
import soundfile as sf
from scipy import signal

class AudioProcessor(QThread):
    progress_updated = pyqtSignal(int)
    result_ready = pyqtSignal(dict)
    visualization_ready = pyqtSignal(object)
    
    def __init__(self, file_path, operation):
        super().__init__()
        self.file_path = file_path
        self.operation = operation
        self.audio_data = None
        self.sr = None
    
    def load_audio(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞"""
        self.audio_data, self.sr = librosa.load(self.file_path, sr=None)
        return self.audio_data, self.sr
    
    def extract_features(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = {}
        
        mfcc = librosa.feature.mfcc(y=self.audio_data, sr=self.sr, n_mfcc=13)
        features['mfcc_mean'] = np.mean(mfcc, axis=1)
        features['mfcc_std'] = np.std(mfcc, axis=1)
        
        spectral_centroid = librosa.feature.spectral_centroid(y=self.audio_data, sr=self.sr)
        features['spectral_centroid'] = float(np.mean(spectral_centroid))
        
        spectral_rolloff = librosa.feature.spectral_rolloff(y=self.audio_data, sr=self.sr)
        features['spectral_rolloff'] = float(np.mean(spectral_rolloff))
        
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=self.audio_data, sr=self.sr)
        features['spectral_bandwidth'] = float(np.mean(spectral_bandwidth))
        
        tempo, _ = librosa.beat.beat_track(y=self.audio_data, sr=self.sr)
        features['tempo'] = float(tempo)
        
        zero_crossing_rate = librosa.feature.zero_crossing_rate(self.audio_data)
        features['zero_crossing_rate'] = float(np.mean(zero_crossing_rate))
        
        rms = librosa.feature.rms(y=self.audio_data)
        features['rms'] = float(np.mean(rms))
        
        chroma = librosa.feature.chroma_stft(y=self.audio_data, sr=self.sr)
        features['chroma_mean'] = np.mean(chroma, axis=1)
        
        return features
    
    def classify_audio(self, features):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∞—É–¥–∏–æ"""
        zcr = features['zero_crossing_rate']
        spectral_centroid = features['spectral_centroid']
        mfcc_std_mean = float(np.mean(features['mfcc_std']))
        tempo = features['tempo']
        
        if zcr > 0.08 and spectral_centroid > 1500 and tempo < 200:
            return "–†–µ—á—å", "–ì–æ–ª–æ—Å —á–µ–ª–æ–≤–µ–∫–∞"
        elif spectral_centroid > 800 and mfcc_std_mean > 30:
            return "–ú—É–∑—ã–∫–∞", "–ú—É–∑—ã–∫–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç"
        elif features['rms'] < 0.005:
            return "–¢–∏—à–∏–Ω–∞", "–û—á–µ–Ω—å —Ç–∏—Ö–∏–π –∑–≤—É–∫"
        elif spectral_centroid < 500:
            return "–ù–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–π —à—É–º", "–ì–ª—É—Ö–∏–µ –∑–≤—É–∫–∏"
        else:
            return "–°–ª–æ–∂–Ω—ã–π –∑–≤—É–∫", "–°–º–µ—à–∞–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
    
    def detect_emotion(self, features):
        """–î–µ—Ç–µ–∫—Ü–∏—è —ç–º–æ—Ü–∏–π –ø–æ –≥–æ–ª–æ—Å—É (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)"""
        spectral_centroid = features['spectral_centroid']
        tempo = features['tempo']
        zcr = features['zero_crossing_rate']
        rms = features['rms']
        
        if spectral_centroid > 2500 and tempo > 120 and rms > 0.05:
            return "–†–∞–¥–æ—Å—Ç—å/–í–æ–∑–±—É–∂–¥–µ–Ω–∏–µ", "–í—ã—Å–æ–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è, –±—ã—Å—Ç—Ä—ã–π —Ç–µ–º–ø"
        elif spectral_centroid < 1500 and tempo < 90 and rms < 0.03:
            return "–ì—Ä—É—Å—Ç—å/–£—Å—Ç–∞–ª–æ—Å—Ç—å", "–ù–∏–∑–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è, –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–µ–º–ø"
        elif zcr > 0.12 and spectral_centroid > 3000 and rms > 0.08:
            return "–ó–ª–æ—Å—Ç—å/–ù–∞–ø—Ä—è–∂–µ–Ω–∏–µ", "–†–µ–∑–∫–∏–π, –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–π –∑–≤—É–∫"
        elif 0.06 < zcr < 0.09 and 1500 < spectral_centroid < 2500:
            return "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ/–°–ø–æ–∫–æ–π–Ω–æ", "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
        else:
            return "–°–º–µ—à–∞–Ω–Ω—ã–µ —ç–º–æ—Ü–∏–∏", "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
    
    def reduce_noise_simple(self):
        """–ü—Ä–æ—Å—Ç–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ —Å –ø–æ–º–æ—â—å—é —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
        nyquist = self.sr / 2
        cutoff = 8000  # Hz
        normal_cutoff = cutoff / nyquist
        
        b, a = signal.butter(4, normal_cutoff, btype='low', analog=False)
        filtered_audio = signal.filtfilt(b, a, self.audio_data)
        
        return filtered_audio
    
    def apply_equalizer(self, low_gain=1.0, mid_gain=1.0, high_gain=1.0):
        """–ü—Ä–æ—Å—Ç–æ–π —ç–∫–≤–∞–ª–∞–π–∑–µ—Ä"""
        # –ù–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã (0-300 Hz)
        b_low, a_low = signal.butter(3, 300/(self.sr/2), btype='low')
        low_freq = signal.filtfilt(b_low, a_low, self.audio_data) * low_gain
        
        # –°—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã (300-3000 Hz)
        b_mid, a_mid = signal.butter(3, [300/(self.sr/2), 3000/(self.sr/2)], btype='band')
        mid_freq = signal.filtfilt(b_mid, a_mid, self.audio_data) * mid_gain
        
        # –í—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã (3000+ Hz)
        b_high, a_high = signal.butter(3, 3000/(self.sr/2), btype='high')
        high_freq = signal.filtfilt(b_high, a_high, self.audio_data) * high_gain
        
        # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
        combined = low_freq + mid_freq + high_freq
        return np.clip(combined, -1.0, 1.0)
    
    def generate_spectrogram(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã"""
        fig, axes = plt.subplots(3, 1, figsize=(12, 10))
        
        # Waveform
        times = np.arange(len(self.audio_data)) / self.sr
        axes[0].plot(times, self.audio_data)
        axes[0].set_title('–í–æ–ª–Ω–æ–≤–∞—è —Ñ–æ—Ä–º–∞')
        axes[0].set_ylabel('–ê–º–ø–ª–∏—Ç—É–¥–∞')
        axes[0].grid(True, alpha=0.3)
        
        # Spectrogram
        D = librosa.amplitude_to_db(np.abs(librosa.stft(self.audio_data)), ref=np.max)
        img = librosa.display.specshow(D, sr=self.sr, x_axis='time', y_axis='hz', 
                                      ax=axes[1], cmap='viridis')
        axes[1].set_title('–°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞')
        axes[1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (Hz)')
        plt.colorbar(img, ax=axes[1])
        
        # MFCC
        mfccs = librosa.feature.mfcc(y=self.audio_data, sr=self.sr, n_mfcc=13)
        librosa.display.specshow(mfccs, sr=self.sr, x_axis='time', ax=axes[2], cmap='coolwarm')
        axes[2].set_title('üéµ MFCC (Mel-Frequency Cepstral Coefficients)')
        axes[2].set_ylabel('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã MFCC')
        axes[2].set_xlabel('–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)')
        plt.colorbar(img, ax=axes[2])
        
        plt.tight_layout()
        return fig
    
    def run(self):
        try:
            results = {}
            
            if self.operation == "analyze":
                self.progress_updated.emit(20)
                self.load_audio()
                
                self.progress_updated.emit(50)
                features = self.extract_features()
                
                self.progress_updated.emit(70)
                audio_type, type_desc = self.classify_audio(features)
                emotion, emotion_desc = self.detect_emotion(features)
                
                self.progress_updated.emit(90)
                results = {
                    'type': 'analysis',
                    'audio_type': audio_type,
                    'type_description': type_desc,
                    'emotion': emotion,
                    'emotion_description': emotion_desc,
                    'features': features,
                    'duration': len(self.audio_data) / self.sr,
                    'sr': self.sr
                }
                
            elif self.operation == "visualize":
                self.load_audio()
                fig = self.generate_spectrogram()
                results = {'type': 'visualization', 'figure': fig}
                
            elif self.operation == "denoise":
                self.load_audio()
                cleaned_audio = self.reduce_noise_simple()
                results = {
                    'type': 'denoise', 
                    'cleaned_audio': cleaned_audio,
                    'sr': self.sr
                }
                
            elif self.operation == "equalize":
                self.load_audio()
                equalized_audio = self.apply_equalizer(1.2, 1.1, 0.9)
                results = {
                    'type': 'equalize',
                    'equalized_audio': equalized_audio,
                    'sr': self.sr
                }
            
            self.progress_updated.emit(100)
            self.result_ready.emit(results)
            
        except Exception as e:
            self.result_ready.emit({'type': 'error', 'message': str(e)})

class AudioMLWorkbench(QMainWindow):
    def __init__(self):
        super().__init__()
        self.current_file = None
        self.initUI()
    
    def initUI(self):
        self.setWindowTitle("ML –º—É–∑—ã–∫–∞")
        self.setGeometry(100, 100, 1200, 800)
        
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        main_layout = QVBoxLayout()
        
        title = QLabel("ML –º—É–∑—ã–∫–∞")
        title.setStyleSheet("font-size: 20px; font-weight: bold; margin: 15px; color: #2E86AB;")
        title.setAlignment(Qt.AlignCenter)
        main_layout.addWidget(title)
        
        file_layout = QHBoxLayout()
        self.select_btn = QPushButton("–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞—É–¥–∏–æ —Ñ–∞–π–ª")
        self.select_btn.clicked.connect(self.select_file)
        self.select_btn.setStyleSheet(self.get_button_style("#2E86AB"))
        
        self.play_btn = QPushButton("‚ñ∂–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏")
        self.play_btn.clicked.connect(self.play_audio)
        self.play_btn.setEnabled(False)
        self.play_btn.setStyleSheet(self.get_button_style("#18A558"))
        
        file_layout.addWidget(self.select_btn)
        file_layout.addWidget(self.play_btn)
        main_layout.addLayout(file_layout)
        
        self.file_label = QLabel("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω")
        self.file_label.setStyleSheet("color: #666; margin: 5px;")
        main_layout.addWidget(self.file_label)
        
        self.progress = QProgressBar()
        self.progress.setVisible(False)
        main_layout.addWidget(self.progress)
        
        self.tabs = QTabWidget()
        
        self.analysis_tab = self.create_analysis_tab()
        self.tabs.addTab(self.analysis_tab, "–ê–Ω–∞–ª–∏–∑")
        
        self.viz_tab = self.create_visualization_tab()
        self.tabs.addTab(self.viz_tab, "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")
        
        self.processing_tab = self.create_processing_tab()
        self.tabs.addTab(self.processing_tab, "–û–±—Ä–∞–±–æ—Ç–∫–∞")
        
        main_layout.addWidget(self.tabs)
        
        self.results_text = QTextEdit()
        self.results_text.setPlaceholderText("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—è–≤—è—Ç—Å—è –∑–¥–µ—Å—å...")
        main_layout.addWidget(self.results_text)
        
        central_widget.setLayout(main_layout)
    
    def get_button_style(self, color):
        return f"""
            QPushButton {{
                background-color: {color};
                color: white;
                border: none;
                padding: 12px 20px;
                font-size: 14px;
                border-radius: 8px;
                margin: 5px;
                min-width: 150px;
            }}
            QPushButton:hover {{
                background-color: #1A5F7A;
            }}
            QPushButton:disabled {{
                background-color: #CCCCCC;
            }}
        """
    
    def create_analysis_tab(self):
        widget = QWidget()
        layout = QVBoxLayout()
        
        group = QGroupBox("–ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ")
        group_layout = QVBoxLayout()
        
        self.analyze_btn = QPushButton("–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        self.analyze_btn.clicked.connect(lambda: self.process_audio("analyze"))
        self.analyze_btn.setStyleSheet(self.get_button_style("#F18F01"))
        group_layout.addWidget(self.analyze_btn)
        
        info = QLabel("–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–≤—É–∫–∞, —ç–º–æ—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏")
        info.setStyleSheet("color: #666; font-size: 12px; margin: 10px;")
        group_layout.addWidget(info)
        
        group.setLayout(group_layout)
        layout.addWidget(group)
        widget.setLayout(layout)
        return widget
    
    def create_visualization_tab(self):
        widget = QWidget()
        layout = QVBoxLayout()
        
        self.viz_btn = QPushButton("–ü–æ–∫–∞–∑–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        self.viz_btn.clicked.connect(lambda: self.process_audio("visualize"))
        self.viz_btn.setStyleSheet(self.get_button_style("#C73E1D"))
        layout.addWidget(self.viz_btn)
        
        info = QLabel("–°—Ç—Ä–æ–∏—Ç –≤–æ–ª–Ω–æ–≤—É—é —Ñ–æ—Ä–º—É, —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É –∏ MFCC")
        info.setStyleSheet("color: #666; font-size: 12px; margin: 10px;")
        layout.addWidget(info)
        
        self.viz_layout = QVBoxLayout()
        layout.addLayout(self.viz_layout)
        
        widget.setLayout(layout)
        return widget
    
    def create_processing_tab(self):
        widget = QWidget()
        layout = QVBoxLayout()
        
        # Denoising
        denoise_group = QGroupBox("–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ")
        denoise_layout = QVBoxLayout()
        self.denoise_btn = QPushButton("–ü–æ–¥–∞–≤–∏—Ç—å —à—É–º")
        self.denoise_btn.clicked.connect(lambda: self.process_audio("denoise"))
        self.denoise_btn.setStyleSheet(self.get_button_style("#3F88C5"))
        denoise_layout.addWidget(self.denoise_btn)
        
        denoise_info = QLabel("–£–±–∏—Ä–∞–µ—Ç –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–π —à—É–º —Å –ø–æ–º–æ—â—å—é —Ñ–∏–ª—å—Ç—Ä–æ–≤")
        denoise_info.setStyleSheet("color: #666; font-size: 12px; margin: 5px;")
        denoise_layout.addWidget(denoise_info)
        
        denoise_group.setLayout(denoise_layout)
        layout.addWidget(denoise_group)
        
        # Equalizer
        eq_group = QGroupBox("–≠–∫–≤–∞–ª–∞–π–∑–µ—Ä")
        eq_layout = QVBoxLayout()
        self.eq_btn = QPushButton("–ü—Ä–∏–º–µ–Ω–∏—Ç—å —ç–∫–≤–∞–ª–∞–π–∑–µ—Ä")
        self.eq_btn.clicked.connect(lambda: self.process_audio("equalize"))
        self.eq_btn.setStyleSheet(self.get_button_style("#44BBA4"))
        eq_layout.addWidget(self.eq_btn)
        
        eq_info = QLabel("–£—Å–∏–ª–∏–≤–∞–µ—Ç –Ω–∏–∑–∫–∏–µ –∏ —Å—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã")
        eq_info.setStyleSheet("color: #666; font-size: 12px; margin: 5px;")
        eq_layout.addWidget(eq_info)
        
        eq_group.setLayout(eq_layout)
        layout.addWidget(eq_group)
        
        widget.setLayout(layout)
        return widget
    
    def select_file(self):
        file_path, _ = QFileDialog.getOpenFileName(
            self, 
            "–í—ã–±–µ—Ä–∏—Ç–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª", 
            "", 
            "Audio Files (*.wav *.mp3 *.flac *.m4a *.ogg)"
        )
        
        if file_path:
            self.current_file = file_path
            self.play_btn.setEnabled(True)
            filename = os.path.basename(file_path)
            self.file_label.setText(f"–ó–∞–≥—Ä—É–∂–µ–Ω: {filename}")
            self.results_text.setText(f"–§–∞–π–ª '{filename}' –≥–æ—Ç–æ–≤ –∫ –∞–Ω–∞–ª–∏–∑—É!\n\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤–æ –≤–∫–ª–∞–¥–∫–∞—Ö –≤—ã—à–µ.")
    
    def play_audio(self):
        if self.current_file:
            try:
                audio_data, sr = librosa.load(self.current_file, sr=None)
                sd.play(audio_data, sr)
                self.results_text.setText("–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ...")
            except Exception as e:
                self.results_text.setText(f"–û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è: {str(e)}")
    
    def process_audio(self, operation):
        if not self.current_file:
            self.results_text.setText("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª!")
            return
        
        self.progress.setVisible(True)
        self.progress.setValue(0)
        self.results_text.setText("–û–±—Ä–∞–±–æ—Ç–∫–∞...")
        
        self.processor = AudioProcessor(self.current_file, operation)
        self.processor.progress_updated.connect(self.progress.setValue)
        self.processor.result_ready.connect(self.handle_results)
        self.processor.start()
    
    def handle_results(self, results):
        self.progress.setVisible(False)
        
        if results['type'] == 'error':
            self.results_text.setText(f"–û—à–∏–±–∫–∞: {results['message']}")
            return
        
        if results['type'] == 'analysis':
            features = results['features']
            
            spectral_centroid = float(features['spectral_centroid'])
            tempo = float(features['tempo'])
            zcr = float(features['zero_crossing_rate'])
            rms = float(features['rms'])
            duration = float(results['duration'])
            sr = int(results['sr'])
            
            text = f"""
–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:

–¢–∏–ø –∞—É–¥–∏–æ: {results['audio_type']}
   {results['type_description']}

–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞: {results['emotion']}
   {results['emotion_description']}

–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
   –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.2f} —Å–µ–∫
   –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏: {sr} –ì—Ü
   Spectral Centroid: {spectral_centroid:.2f} Hz
   Tempo: {tempo:.1f} BPM
   Zero Crossing Rate: {zcr:.4f}
   RMS Energy: {rms:.4f}

–ê—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!
"""
            self.results_text.setText(text)
        
        elif results['type'] == 'visualization':
            for i in reversed(range(self.viz_layout.count())): 
                widget = self.viz_layout.itemAt(i).widget()
                if widget:
                    widget.setParent(None)
            
            canvas = FigureCanvas(results['figure'])
            self.viz_layout.addWidget(canvas)
            self.results_text.setText("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã! –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫–µ '–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è'")
        
        elif results['type'] == 'denoise':
            base_name = os.path.splitext(self.current_file)[0]
            output_path = f"{base_name}_cleaned.wav"
            sf.write(output_path, results['cleaned_audio'], int(results['sr']))
            self.results_text.setText(f"""
–®–£–ú–û–ü–û–î–ê–í–õ–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!

–û—á–∏—â–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {os.path.basename(output_path)}

–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω, –Ω–æ–≤—ã–π —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º '_cleaned'
–í—ã –º–æ–∂–µ—Ç–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –æ–±–∞ —Ñ–∞–π–ª–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è!
""")
        
        elif results['type'] == 'equalize':
            base_name = os.path.splitext(self.current_file)[0]
            output_path = f"{base_name}_equalized.wav"
            sf.write(output_path, results['equalized_audio'], int(results['sr']))
            self.results_text.setText(f"""
–≠–ö–í–ê–õ–ê–ô–ó–ï–† –ü–†–ò–ú–ï–ù–ï–ù!

–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {os.path.basename(output_path)}

–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫–≤–∞–ª–∞–π–∑–µ—Ä–∞:
   - –ù–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã: +20%
   - –°—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã: +10% 
   - –í—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã: -10%

–°—Ä–∞–≤–Ω–∏—Ç–µ –æ—Ä–∏–≥–∏–Ω–∞–ª –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é!
""")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = AudioMLWorkbench()
    window.show()
    sys.exit(app.exec_())